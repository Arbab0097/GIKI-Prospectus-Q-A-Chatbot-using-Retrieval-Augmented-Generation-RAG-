{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\mq\\anaconda\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\mq\\anaconda\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: chromadb in c:\\users\\mq\\anaconda\\lib\\site-packages (1.0.20)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mq\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain) (0.4.17)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mq\\anaconda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\mq\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mq\\anaconda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mq\\anaconda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mq\\anaconda\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mq\\anaconda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\mq\\anaconda\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (0.16.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from chromadb) (4.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mq\\anaconda\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\mq\\anaconda\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\mq\\anaconda\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mq\\anaconda\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\mq\\anaconda\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mq\\anaconda\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mq\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\mq\\anaconda\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\mq\\anaconda\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\mq\\anaconda\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\mq\\anaconda\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\mq\\anaconda\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mq\\anaconda\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\mq\\anaconda\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\mq\\anaconda\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\mq\\anaconda\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\mq\\anaconda\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\mq\\anaconda\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\mq\\anaconda\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mq\\anaconda\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\mq\\anaconda\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\mq\\anaconda\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mq\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mq\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\mq\\anaconda\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\mq\\anaconda\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\mq\\anaconda\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\mq\\anaconda\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\mq\\anaconda\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mq\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mq\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m Chroma(embedding_function\u001b[38;5;241m=\u001b[39membedding_model)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Add documents to the vector store\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39mchunks, ids\u001b[38;5;241m=\u001b[39muuids)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot initialized with Hugging Face embeddings!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:286\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    284\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 286\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    287\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:286\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    284\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 286\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    287\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# # Install if not already installed\n",
    "# !pip install sentence-transformers langchain chromadb\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from uuid import uuid4\n",
    "\n",
    "# # Initialize a free embedding model\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Example: your documents for chatbot knowledge base\n",
    "# chunks = [\n",
    "#     \"Hello! I am your assistant.\",\n",
    "#     \"You can ask me questions based on my knowledge base.\"\n",
    "# ]\n",
    "# uuids = [str(uuid4()) for _ in chunks]\n",
    "\n",
    "# # Initialize Chroma vector store\n",
    "# vector_store = Chroma(embedding_function=embedding_model)\n",
    "\n",
    "# # Add documents to the vector store\n",
    "# vector_store.add_documents(documents=chunks, ids=uuids)\n",
    "\n",
    "# print(\"Chatbot initialized with Hugging Face embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = r\"data\"\n",
    "# CHROMA_PATH = r\"chroma_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initiate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect to the chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = Chroma(\n",
    "#     collection_name=\"example_collection\",\n",
    "#     embedding_function=embedding_model,  \n",
    "#     persist_directory=CHROMA_PATH,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the vectorstore to be the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_results = 5\n",
    "# retriever = vector_store.as_retriever(search_kwargs={'k': num_results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call this function for every message added to the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stream_response(message, history):\n",
    "#     #print(f\"Input: {message}. History: {history}\\n\")\n",
    "\n",
    "#     # retrieve the relevant chunks based on the question asked\n",
    "#     docs = retriever.invoke(message)\n",
    "\n",
    "#     # add all the chunks to 'knowledge'\n",
    "#     knowledge = \"\"\n",
    "#     for doc in docs:\n",
    "#         knowledge += doc.page_content+\"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "# # Initialize Hugging Face client (replace with your model + token)\n",
    "# import os\n",
    "# client = InferenceClient(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "# )\n",
    "\n",
    "# def stream_response(message, history):\n",
    "#     knowledge = \"...\"  # replace with retrieved knowledge\n",
    "    \n",
    "#     rag_prompt = f\"\"\"\n",
    "#     You are an assistant which answers questions based on knowledge which is provided to you.\n",
    "#     While answering, you don't use your internal knowledge,\n",
    "#     but solely the information in the \"The knowledge\" section.\n",
    "#     You don't mention anything to the user about the provided knowledge.\n",
    "\n",
    "#     The question: {message}\n",
    "#     Conversation history: {history}\n",
    "#     The knowledge: {knowledge}\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(\"Prompt sent to LLM:\\n\", rag_prompt)\n",
    "\n",
    "#     partial_message = \"\"\n",
    "\n",
    "#     response = llm.invoke(user_input)\n",
    "#     return response\n",
    "\n",
    "\n",
    "# # Gradio ChatInterface\n",
    "# chatbot = gr.ChatInterface(\n",
    "#     fn=stream_response,\n",
    "#     textbox=gr.Textbox(\n",
    "#         placeholder=\"Send to the LLM...\",\n",
    "#         container=False,\n",
    "#         autoscroll=True,\n",
    "#         scale=7,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# chatbot.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initiate the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "launch the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sent to LLM:\n",
      " \n",
      "    You are an assistant which answers questions based ONLY on the provided knowledge.\n",
      "    The question: Hi please tell me something about gilki\n",
      "    Conversation history: []\n",
      "    The knowledge: supportive faculty and vibrant student life, filled with \n",
      "diverse clubs and activities, have made my time here \n",
      "truly unforgettable. GIKI is more than just a place \n",
      "of learning; it’s a community that fosters innovation, \n",
      "leadership, and lifelong friendships.\n",
      "ABDUL REHMAN\n",
      "\n",
      "from our professors was invaluable, guiding me through \n",
      "complex concepts and encouraging me to push the \n",
      "boundaries of my understanding.\n",
      "GIKI is a melting pot of people from different cultures \n",
      "in Pakistan. This diversity not only broadened my\n",
      "\n",
      "As I continue my journey at GIKI, I am confident that \n",
      "the skills and experiences gained here will pave the \n",
      "way for future success.”\n",
      "HARIS ZEB \n",
      "(POWER)\n",
      "\n",
      "will contribute to the advancement of Pakistan and beyond. \n",
      "Institutions like GIKI, with a legacy of three decades of excellence, serve as \n",
      "pillars of progress and innovation. They must act as a catalyst for transformative change, nurturing visionary\n",
      "\n",
      "as the engineering marvels we’ve learned to create \n",
      "within GIKI. But beyond the academic rigor and \n",
      "technical prowess, it is the intangible essence of GIKI \n",
      "that truly sets it apart. It is the camaraderie forged \n",
      "during late-night study sessions and the bonds formed\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\route_utils.py\", line 349, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\blocks.py\", line 2274, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\blocks.py\", line 1779, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\utils.py\", line 876, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\chat_interface.py\", line 551, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\gradio\\chat_interface.py\", line 925, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\AppData\\Local\\Temp\\ipykernel_18756\\1202179209.py\", line 45, in stream_response\n",
      "    response = client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 2376, in text_generation\n",
      "    request_parameters = provider_helper.prepare_request(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py\", line 90, in prepare_request\n",
      "    provider_mapping_info = self._prepare_mapping_info(model)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MQ\\anaconda\\Lib\\site-packages\\huggingface_hub\\inference\\_providers\\_common.py\", line 159, in _prepare_mapping_info\n",
      "    raise ValueError(\n",
      "ValueError: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider novita. Supported task: conversational.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from huggingface_hub import InferenceClient\n",
    "from uuid import uuid4\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Load environment variables (for HuggingFace token)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings and vector store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=CHROMA_PATH,\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "# Initialize HuggingFace client\n",
    "client = InferenceClient(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    ")\n",
    "\n",
    "# Chat handler\n",
    "def stream_response(message, history):\n",
    "    # Retrieve relevant docs\n",
    "    docs = retriever.invoke(message)\n",
    "    knowledge = \"\\n\\n\".join([doc.page_content for doc in docs]) or \"No context found.\"\n",
    "\n",
    "    # Build RAG prompt\n",
    "    rag_prompt = f\"\"\"\n",
    "    You are an assistant which answers questions based ONLY on the provided knowledge.\n",
    "    The question: {message}\n",
    "    Conversation history: {history}\n",
    "    The knowledge: {knowledge}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Prompt sent to LLM:\\n\", rag_prompt)\n",
    "\n",
    "    # Send to HuggingFace model\n",
    "    def stream_response(message, history):\n",
    "    # Retrieve relevant docs\n",
    "        docs = retriever.invoke(message)\n",
    "        knowledge = \"\\n\\n\".join([doc.page_content for doc in docs]) or \"No context found.\"\n",
    "\n",
    "    # Build RAG prompt\n",
    "        rag_prompt = f\"\"\"\n",
    "        You are an assistant which answers questions based ONLY on the provided knowledge.\n",
    "        The question: {message}\n",
    "        Conversation history: {history}\n",
    "        The knowledge: {knowledge}\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Prompt sent to LLM:\\n\", rag_prompt)\n",
    "\n",
    "    # Use Hugging Face conversational API\n",
    "        response = client.conversational(\n",
    "            {\n",
    "                \"inputs\": {\n",
    "                \"past_user_inputs\": [h[0] for h in history],\n",
    "                \"generated_responses\": [h[1] for h in history],\n",
    "                \"text\": rag_prompt,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return response.generated_text\n",
    "\n",
    "\n",
    "# Gradio ChatInterface\n",
    "        chatbot = gr.ChatInterface(\n",
    "            fn=stream_response,\n",
    "            textbox=gr.Textbox(\n",
    "                placeholder=\"Ask me anything...\",\n",
    "                container=False,\n",
    "                autoscroll=True,\n",
    "                scale=7,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "chatbot.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
